# Entropy
Oftentimes we might think that more organized looking configurations of particles are less probably. This isn't really case, they are just as likely as any other probability that looks more chaotic. It just happens that there are for more chaotic probabilities than there or "organized" states.

Imagine some arbitrary macrostate defined by the extensive variables $\varepsilon=(N, E, V, \dots)$ and $\Omega(\varepsilon)$ to be the number of microstates compatible with the macro constraints.

Boltzmann defines entropy as $S=k\ln(\Omega(\varepsilon))$ where $k$ is some proportionality constant.
- The logarithm present in the definitely makes $S$ an extensive variables that scales linearly with the system size.

*Equilibrium* is defined to be the state when $S$ is at its maximum

Second Law of Thermodynamics
> Overtime, the entropy of a closed system never decreases
